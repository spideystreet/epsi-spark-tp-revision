# TP Spark - Manipulation de DonnÃ©es avec Scala et SBT

Ce projet est un exercice pratique de manipulation de donnÃ©es avec Apache Spark, rÃ©alisÃ© dans le cadre d'un TP pour EPSI. L'application charge un fichier CSV, effectue des transformations de base sur les donnÃ©es (filtrage, agrÃ©gation) et sauvegarde les rÃ©sultats au format Parquet.

## âœ¨ FonctionnalitÃ©s

- **Chargement de donnÃ©es** : Lit un fichier `ventes_fictives.csv`.
- **Affichage du SchÃ©ma** : Affiche la structure et les types de donnÃ©es du DataFrame.
- **Filtrage** : SÃ©lectionne les enregistrements oÃ¹ la quantitÃ© vendue est supÃ©rieure Ã  30.
- **AgrÃ©gation** : Calcule le revenu total par rÃ©gion.
- **Sauvegarde** : Enregistre les DataFrames filtrÃ©s et agrÃ©gÃ©s au format Parquet dans le dossier `output/`.

## ğŸ› ï¸ Technologies UtilisÃ©es

- **Scala** : `2.12.18`
- **SBT** : `1.11.2`
- **Apache Spark** : `3.5.0`

## ğŸ“‚ Structure du Projet

```
.
â”œâ”€â”€ build.sbt                   # Fichier de configuration SBT avec les dÃ©pendances
â”œâ”€â”€ data
â”‚   â””â”€â”€ ventes_fictives.csv       # DonnÃ©es d'entrÃ©e
â”œâ”€â”€ output/                     # Dossier de sortie pour les fichiers Parquet
â”œâ”€â”€ project
â”‚   â””â”€â”€ build.properties        # PropriÃ©tÃ©s du build SBT
â”œâ”€â”€ src/main/scala
â”‚   â””â”€â”€ ScalaApp.scala          # Code source de l'application Spark
â””â”€â”€ .sbtopts                    # Options de la JVM pour la compatibilitÃ© Java
```

## ğŸš€ Comment l'exÃ©cuter

### PrÃ©requis

- Java 11 ou supÃ©rieur (ce projet a Ã©tÃ© testÃ© avec Java 17)
- SBT (Simple Build Tool)

### Lancer l'application

Pour compiler et exÃ©cuter l'application, utilisez la commande suivante Ã  la racine du projet :

```bash
sbt run
```

Le script s'exÃ©cutera, affichera les diffÃ©rents DataFrames dans la console et crÃ©era les fichiers Parquet dans le rÃ©pertoire `output/`.